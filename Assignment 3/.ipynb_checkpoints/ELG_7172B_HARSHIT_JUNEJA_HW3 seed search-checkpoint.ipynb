{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ELG 7172B Assignment 3</h1>\n",
    "<h3>Submitted by: Harshit Juneja<br>Student ID 300074170</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Problem 1</h4><br>\n",
    "Logistic regression with the first and second order gradient descent implemented in Python is presented\n",
    "here: https://github.com/atcemgil/notes/blob/master/LogisticRegression.ipynb . Analyze the advantages of\n",
    "using the second order gradient descent.<br><br>\n",
    "Next, perform logistic regression on one of the following realistic data set for logistic regression\n",
    "(https://data.princeton.edu/wws509/datasets/#copen) and compare results obtained using different\n",
    "gradient descent algorithms implemented in NumPy or Tensorflow.\n",
    "Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><u>Solution to Question 1</u></h4><br>\n",
    "\n",
    "For our demonstration of logistic regression using first and second order methods we will be using the Housing Conditions in Copenhagen dataset, which is posted at [1] and comes from [3]. As per the [1], the data is in a four-way table format classifying 1681 residents of twelve areas in Copenhagen. We have converted the data into a tab-delimited file and rearranged the columns slightly. The columns, in order are:\n",
    "* The number of respondents who gave the reponse\n",
    "* The type of housing (1=tower blocks, 2=apartments, 3=atrium houses and 4=terraced houses)\n",
    "* Respondent's feeling of influence on apartment management (1=low, 2=medium,3=high),\n",
    "* Respondent's satisfaction with housing conditions (1=low, 2=medium, 3=high)\n",
    "* Respondent's degree of contact with neighbors (1=low, 2=high)\n",
    "\n",
    "We will read this file into python using Numpy, and preprocess the data to perform logistic regression on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split # we will use this to randomly split our data into two sets\n",
    "from time import perf_counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Housing data matrix shape (72, 5)\n",
      "First five rows of housing dataset\n",
      "number_respondents housing_type influence satisfaction neighbour_contact\n",
      " [[21.  1.  1.  1.  1.]\n",
      " [21.  1.  1.  2.  1.]\n",
      " [28.  1.  1.  3.  1.]\n",
      " [14.  1.  1.  1.  2.]\n",
      " [19.  1.  1.  2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "#reading the file\n",
    "housing_data = np.genfromtxt('housing_data.tsv', delimiter='\\t')\n",
    "housing_data = housing_data[1:,] #drop the first row (headers)\n",
    "print(\"Housing data matrix shape\", housing_data.shape)\n",
    "print(\"First five rows of housing dataset\\nnumber_respondents housing_type\",\n",
    "      \"influence satisfaction neighbour_contact\\n\",  housing_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will un-roll the data by removing the first column, and copying rows as per the number in the first column to get a matrix with 1681 rows and 5 columns. The first column will be all ones for our training the bias weight (this is an \"artificial\" feature added to the model to make it more robust)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_unrolled = np.ones((1681,5)) #preallocation\n",
    "k = 0\n",
    "for i in range(housing_data.shape[0]):\n",
    "    num_respondents = int(housing_data[i][0])\n",
    "    for j in range(num_respondents):\n",
    "        housing_data_unrolled[k][1:5] = housing_data[i][1:5]\n",
    "        k += 1\n",
    "housing_data_unrolled[:,4] -= 1 #change labels for neighbour contact (this will be our predicted variable)\n",
    "housing_data_unrolled_input = housing_data_unrolled[:,0:4]\n",
    "housing_data_unrolled_output = housing_data_unrolled[:,4].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression model is\n",
    "\n",
    "\\begin{align}\n",
    "y = f(z) = \\frac{1}{1 + e^{-z}}\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "where, \n",
    "\n",
    "\\begin{align}\n",
    "z = b + w_1 * \\textrm{housing_type} + w_2 * \\textrm{influence} + w_3 * \\textrm{satisfaction}\\tag{2}\n",
    "\\end{align}\n",
    "<br>Here, $y$ is our predicted variable, whether a person has low neighbour contact (0) or high neighbour contact and $b$ is the bias term. Combining all the inputs together as matrix x, we can rewrite z as<br>\n",
    "\n",
    "\\begin{align}\n",
    "z_i = w^Tx_i + b \\tag{3}\n",
    "\\end{align}\n",
    "\n",
    "$w$ is $3{\\times}1$ vector, $w = \\begin{smallmatrix} [w_1 & w_2 & w_3]^T\\end{smallmatrix}$ and $x = \\begin{smallmatrix}[\\textrm{housing_type}_i & \\textrm{influence}_i & \\textrm{satisfaction}_i] \\end{smallmatrix}$ where $i$ is sample number.<br>\n",
    "\n",
    "To incorporate the bias term into matrix multiplication, we can rewrite $x_i$ as $\\mathbf{x_i} = \\begin{smallmatrix}[1 & \\textrm{housing_type}_i & \\textrm{influence}_i & \\textrm{satisfaction}_i] \\end{smallmatrix}$ and $w$ as $\\mathbf{w} = \\begin{smallmatrix} [w_0 & w_1 & w_2 & w_3]^T\\end{smallmatrix}$ where $w_0 = b$.<br> \n",
    "Further, we can take all our inputs together as a matrix of size $n{\\times}m$, where $n$ is the number of features per sample, and $m$ is the number of input samples. We will label this matrix as $\\mathbf{X}$. In our chosen problem, $n=4$ (3 features, plus 1 term for bias) and $m=1260$ (size of training subset).\n",
    "\n",
    "We can now rewrite equation $(3)$, where $\\mathbf{z}$ holds a row vector of size $1{\\times}m$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z} = \\mathbf{w}^T\\mathbf{X} \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "<b>Maximum Likelihood estimation and loss function</b>\n",
    "\n",
    "\\begin{align}\n",
    "P(y_i=1|\\mathbf{x}_i;\\mathbf{w}) = h_\\mathbf{w}(\\mathbf{x}_i) \\tag{5.1}\\\\\n",
    "P(y_i=0|\\mathbf{x}_i;\\mathbf{w}) = 1 - h_\\mathbf{w}(\\mathbf{x}_i) \\tag{5.2}\\\\\n",
    "\\end{align}\n",
    "<br>\n",
    "Here $h_\\mathbf{w}(x)$ is the hypothesis function, telling us the probability of $y_i$ = 1, with weights w<br>\n",
    "\n",
    "These can be combined as\n",
    "\\begin{align}\n",
    "P(y_i|\\mathbf{x}_i;\\mathbf{w}) = h_\\mathbf{w}(\\mathbf{x}_i)^{y_i} [1 - h_\\mathbf{w}(\\mathbf{x}_i)]^{(1-y_i)} \\tag{6}\\\\\n",
    "\\end{align}\n",
    "\n",
    "The likelihood function is defined as\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) = \\prod_{i=1}^{m} P({y}_i|\\mathbf{x}_i;\\mathbf{w}) \\tag{7}\\\\\n",
    "L(\\mathbf{w}) = \\prod_{i=1}^{m} h_\\mathbf{w}(\\mathbf{x}_i)^{y_i} [1 - h_\\mathbf{w}(\\mathbf{x}_i)]^{(1-y_i)} \\tag{8} \\\\\n",
    "\\end{align}\n",
    "To get the best predictions, we need to maximize the likelihood function. Taking log on both sides, the log likelihood is given as:\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) = \\sum_{i=1}^{m} y_i * log(h_\\mathbf{w}(\\mathbf{x}_i)) + (1-y_i) * log( 1 - h_\\mathbf{w}(\\mathbf{x}_i)) \\tag{9} \\\\\n",
    "\\end{align}\n",
    "This is a concave function. To use convex optimization, we define a Loss function which is the average of  the negative of log likelihood and minimize it.\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\mathbf{w}) = -\\frac{1}{m}[y_i * log(h_\\mathbf{w}(\\mathbf{x}_i)) + (1-y_i) * log( 1 - h_\\mathbf{w}(\\mathbf{x}_i))] \\tag{10}) \\\\\n",
    "\\end{align}\n",
    "Our gradients are\n",
    "\\begin{align}\n",
    "\\nabla\\mathcal{L} = \\frac{1}{m} \\sum_{1}^{m}(h_\\mathbf{w}(\\mathbf{x}_i) - y_i)\\mathbf{x}_i \\tag{11}\\\\\n",
    "\\end{align}\n",
    "Our weight update equation is\n",
    "\\begin{align}\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla\\mathcal{L} \\tag{12}\\\\\n",
    "\\end{align}\n",
    "<br>Here $\\eta$ is the learning rate.<br>\n",
    "<b>This is simple gradient descent - a type of first order optimization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 True\n",
      "total time =  6.7975186230014515\n",
      "192 True\n",
      "total time =  7.99921408299997\n",
      "total time =  6.110532454000349\n",
      "total time =  7.4741281810001965\n",
      "total time =  7.505333781002264\n",
      "total time =  8.446914837000804\n",
      "total time =  7.6733409839980595\n",
      "771 True\n",
      "total time =  8.86902226500024\n",
      "total time =  7.611032410000917\n",
      "total time =  8.165163599001971\n",
      "best result =  257 best seed =  771\n",
      "best total =  1028 best seed =  224\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "best_seed = -1\n",
    "best_count = -1\n",
    "best_result = -1\n",
    "best_result_seed = -1\n",
    "for seed in range(0,1000):\n",
    "    #split data into training and validation(test) sets, 75:25\n",
    "    housing_data_train_input, housing_data_test_input, housing_data_train_output, housing_data_test_output = train_test_split(housing_data_unrolled_input, housing_data_unrolled_output, test_size=0.25, random_state=seed, shuffle=True)\n",
    "    #to get training input into 4x1260 matrix\n",
    "    housing_data_train_input = housing_data_train_input.T\n",
    "    #to get test input into 4x1260 matrix\n",
    "    housing_data_test_input = housing_data_test_input.T\n",
    "    #print(\"Training subset matrix size = \",housing_data_train_input.shape)\n",
    "    #print(\"Test subset matrix size = \",housing_data_test_input.shape)\n",
    "    #best random state = 2\n",
    "    prng = np.random.RandomState(42) #seeding prng for repeatable results\n",
    "    w = prng.rand(4,1) #random initialization of weights\n",
    "    learning_rate = 0.01\n",
    "    x = housing_data_train_input\n",
    "    y = housing_data_train_output.T\n",
    "    m = housing_data_train_input.shape[1]\n",
    "    epochs = 500000 # upto 1,000,000 iterations\n",
    "    convergence_threshold = 1e-17\n",
    "    error = np.zeros((epochs,2))\n",
    "    #simple gradient descent - no momentum\n",
    "    prev_error = 0\n",
    "    time_start = perf_counter()\n",
    "    for i in range(epochs):\n",
    "        z = np.dot(w.T,x)\n",
    "        predicted_y = sigmoid(z)\n",
    "        error[i] = np.array([i, np.sum( -y*np.log(predicted_y) - (1-y)*np.log(1-predicted_y) ) / m])\n",
    "        curr_error = error[i,1]\n",
    "        if i > epochs/100 and i > 100 and abs(abs(curr_error) - abs(prev_error)) < convergence_threshold:\n",
    "            #print(\"Convergence threshold reached at epoch\", i)\n",
    "            break\n",
    "        #if(i % (epochs/10) == 0):\n",
    "            #print(\"[epoch\\t error] = \", error[i],\"elapsed time \", perf_counter()-time_start)\n",
    "        #calculate gradients\n",
    "        grad_w = (1/m) * np.sum((predicted_y - y)*x, axis = 1)\n",
    "        #calculate weight update\n",
    "        delta_w = learning_rate * grad_w\n",
    "        #update weights\n",
    "        w = w - delta_w.reshape(-1,1)\n",
    "        prev_error = curr_error\n",
    "    if (seed+1)%100 == 0:\n",
    "        print(\"total time = \", perf_counter()-time_start)\n",
    "    #print(w)\n",
    "    \n",
    "    z = np.dot(w.T,housing_data_train_input)\n",
    "    y2 = sigmoid(z)\n",
    "    for i in range(y2.shape[1]):\n",
    "        if y2[0,i] > 0.5:\n",
    "            y2[0,i] = 1\n",
    "        else:\n",
    "            y2[0,i] = 0\n",
    "    correct_prediction = y2 == housing_data_train_output.T\n",
    "    unique, train_counts = np.unique(correct_prediction, return_counts=True)\n",
    "    train_acc = train_counts[1]/1260\n",
    "    #print(\"Summary of prediction results\\n\", unique, \"\\n\", train_counts, \"\\ntrain accuracy = \", train_acc)\n",
    "    \n",
    "    z = np.dot(w.T,housing_data_test_input)\n",
    "    y2 = sigmoid(z)\n",
    "    for i in range(y2.shape[1]):\n",
    "        if y2[0,i] > 0.5:\n",
    "            y2[0,i] = 1\n",
    "        else:\n",
    "            y2[0,i] = 0\n",
    "    correct_prediction = y2 == housing_data_test_output.T\n",
    "    unique, test_counts = np.unique(correct_prediction, return_counts=True)\n",
    "    test_acc = test_counts[1]/421\n",
    "    #print(\"Summary of prediction results\\n\", unique, \"\\n\", test_counts, \"\\ntest accuracy = \", test_acc)\n",
    "    if train_acc <0.5 or test_acc <0.5:\n",
    "        print(\"Warning!\")\n",
    "    if train_acc/test_acc > 1 and train_acc > 0.6 and test_acc > 0.6 and test_counts[1] > best_count:\n",
    "        print(seed, test_acc>0.6)\n",
    "        best_count = test_counts[1]\n",
    "        best_seed = seed\n",
    "        \n",
    "    combined_count = train_counts[1]+test_counts[1]\n",
    "    if combined_count > best_result:\n",
    "        best_result = combined_count\n",
    "        best_result_seed = seed\n",
    "        \n",
    "print(\"best result = \",best_count, \"best seed = \", best_seed)\n",
    "print(\"best total = \",best_result, \"best seed = \", best_result_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of prediction results\n",
      " [False  True] \n",
      " [173 248]\n"
     ]
    }
   ],
   "source": [
    "z = np.dot(w.T,housing_data_test_input)\n",
    "y2 = sigmoid(z)\n",
    "for i in range(y2.shape[1]):\n",
    "    if y2[0,i] > 0.5:\n",
    "        y2[0,i] = 1\n",
    "    else:\n",
    "        y2[0,i] = 0\n",
    "correct_prediction = y2 == housing_data_test_output.T\n",
    "unique, counts = np.unique(correct_prediction, return_counts=True)\n",
    "print(\"Summary of prediction results\\n\", unique, \"\\n\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch\t error] =  [0.         2.18702563] elapsed time  0.0007021139999778825\n",
      "[epoch\t error] =  [5.00000000e+04 6.66843401e-01] elapsed time  47.86163257599992\n",
      "[epoch\t error] =  [1.00000000e+05 6.66798681e-01] elapsed time  93.67883888000006\n",
      "[epoch\t error] =  [1.50000000e+05 6.66795142e-01] elapsed time  139.94583717900014\n",
      "[epoch\t error] =  [2.00000000e+05 6.66794862e-01] elapsed time  187.6932658830001\n",
      "[epoch\t error] =  [2.50000000e+05 6.66794839e-01] elapsed time  233.807161146\n",
      "[epoch\t error] =  [3.00000000e+05 6.66794838e-01] elapsed time  281.30426955899975\n",
      "[epoch\t error] =  [3.50000000e+05 6.66794838e-01] elapsed time  327.30454560299995\n",
      "Convergence threshold reached at epoch 367084\n",
      "total time =  344.0396573580001\n",
      "-0.27220161720913505 0.2951677654583766 -0.32392145781709636 0.23722074856808684\n"
     ]
    }
   ],
   "source": [
    "def logistic(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "admit = housing_data_train_output\n",
    "gre = housing_data_train_input.T[:,1]\n",
    "gpa = housing_data_train_input.T[:,2]\n",
    "rank = housing_data_train_input.T[:,3]\n",
    "admit = admit.reshape(m,)\n",
    "prng = np.random.RandomState(42) #seeding prng for repeatable results\n",
    "beta0 = prng.rand()\n",
    "beta1 = prng.rand()\n",
    "beta2 = prng.rand()\n",
    "beta3 = prng.rand()\n",
    "learning_rate = 0.002 #use this for all methods to achieve comparable results\n",
    "n = m\n",
    "epochs = 500000 # upto 1,000,000 iterations\n",
    "convergence_threshold = 1e-17\n",
    "error = np.zeros((epochs,2))\n",
    "#gradient descent with no momentum\n",
    "prev_error = 0\n",
    "start_time = perf_counter()\n",
    "for i in range(epochs):\n",
    "    z = beta0 + beta1*gre + beta2*gpa + beta3*rank\n",
    "    predicted_admit = logistic(z)\n",
    "    error[i] = np.array([i, np.sum([data for data in (-admit*np.log(predicted_admit)) - (1-admit)*np.log(1-predicted_admit)]) / n])\n",
    "    curr_error = error[i,1]\n",
    "    if i > epochs/100 and i > 100 and abs(abs(curr_error) - abs(prev_error)) < convergence_threshold:\n",
    "        print(\"Convergence threshold reached at epoch\", i)\n",
    "        break\n",
    "    if(i % (epochs/10) == 0):\n",
    "        print(\"[epoch\\t error] = \", error[i],\"elapsed time \", perf_counter()-start_time)\n",
    "    #calculate gradients\n",
    "    d_beta0 = (1/n) * sum((predicted_admit - admit))\n",
    "    d_beta1 = (1/n) * sum((predicted_admit - admit)*gre)\n",
    "    d_beta2 = (1/n) * sum((predicted_admit - admit)*gpa)\n",
    "    d_beta3 = (1/n) * sum((predicted_admit - admit)*rank)\n",
    "    #update vectors\n",
    "    delta_beta0 = learning_rate * d_beta0\n",
    "    delta_beta1 = learning_rate * d_beta1\n",
    "    delta_beta2 = learning_rate * d_beta2\n",
    "    delta_beta3 = learning_rate * d_beta3\n",
    "    #update parameters\n",
    "    beta0 = beta0 - delta_beta0\n",
    "    beta1 = beta1 - delta_beta1\n",
    "    beta2 = beta2 - delta_beta2\n",
    "    beta3 = beta3 - delta_beta3\n",
    "    prev_error = curr_error\n",
    "    \n",
    "print(\"total time = \", perf_counter()-start_time)\n",
    "print(beta0, beta1, beta2, beta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.27220161720913505 0.2951677654583766 -0.32392145781709636 0.23722074856808684\n",
      "Summary of prediction results\n",
      " [False  True] \n",
      " [153 268]\n"
     ]
    }
   ],
   "source": [
    "gre_test = housing_data_test_input.T[:,1]\n",
    "gpa_test = housing_data_test_input.T[:,2]\n",
    "rank_test = housing_data_test_input.T[:,3]\n",
    "z = beta0 + beta1*gre_test + beta2*gpa_test + beta3*rank_test\n",
    "print(beta0, beta1, beta2, beta3)\n",
    "predicted_admit_test = logistic(z)\n",
    "for i in range(len(predicted_admit_test)):\n",
    "    if predicted_admit_test[i] > 0.5:\n",
    "        predicted_admit_test[i] = 1\n",
    "    else:\n",
    "        predicted_admit_test[i] = 0\n",
    "correct_prediction = predicted_admit_test == housing_data_test_output.T\n",
    "unique, counts = np.unique(correct_prediction, return_counts=True)\n",
    "print(\"Summary of prediction results\\n\", unique, \"\\n\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Analysis of Second Order Optimization</h4>\n",
    "\n",
    "As listed in [], the advantages of Newton's method are:\n",
    "* This method allows us to take advantage of properties of the loss function beyond just the gradient.\n",
    "* The Hessian matrix describes the local curvature of the function, which allows us to make a more efficient update resulting in faster convergence to the local minima\n",
    "* It does not use a learning rate. We can calculate the optimal step size using line tracking or approximate the same using Armijo backtracking []. Thus, we have one less hyperparameter to tune\n",
    "\n",
    "However, this method is not without disadvantages, the major challenges being that the Hessian matrix has $n^2$ terms, where n is the number of parameters of the loss function, and further, matrix inversion is a $O(n^2.81)$ operation. []\n",
    "\n",
    "In our logistic regression problem, we observed the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
